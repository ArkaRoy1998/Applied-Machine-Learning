{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c380512a-c54a-4699-b9bd-70c37317bbe6",
   "metadata": {},
   "source": [
    "### Arka Roy- MDS202311\n",
    "\n",
    "### Prepare.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7527d-2058-4aa0-96da-f7921a9b74f4",
   "metadata": {},
   "source": [
    "### Function to load data from a given file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4320da-eed7-4c2c-bd13-6439a4b0e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arkaroy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/arkaroy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/arkaroy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b04f213-ab49-45b2-95fd-6df345a94a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to load dataset \n",
    "def load_sms_spam_dataset(file_path):\n",
    "   \n",
    "    try:\n",
    "        data = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'message'], encoding='utf-8')\n",
    "        print(f\"Dataset loaded successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the file path.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: Parsing issue encountered. Ensure the file format is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf634cc-2037-4480-808c-0a613c12c0c4",
   "metadata": {},
   "source": [
    "### Function for text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738767db-78fc-4bd6-88c0-f7cad1904fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text preprocessing function\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = ' '.join(text.split())  # Remove extra whitespace\n",
    "\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    stop_words = set(stopwords.words('english'))  # Load stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e403d9a-996b-4ff5-9b36-fb8fbc0c6511",
   "metadata": {},
   "source": [
    "### Function for pre-processing the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c5a17f-70a5-49b0-bccb-99d72a7c76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the entire dataset\n",
    "def preprocess_dataset(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    data = data.copy()\n",
    "    data['message'] = data['message'].apply(preprocess_text)\n",
    "\n",
    "    # Convert labels to numeric (0 for ham, 1 for spam)\n",
    "    data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b4816-a287-4b68-8b76-d481fed18047",
   "metadata": {},
   "source": [
    "### Function to split and save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dcef16c-de15-4927-800b-92444e78a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_and_save_data(data: pd.DataFrame, output_dir: str, train_ratio=0.8, val_ratio=0.1, random_state=42):\n",
    "   \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Splitting into train and temp set (val + test)\n",
    "    y = data['label']\n",
    "    X = data.drop(columns=['label'])\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_ratio), stratify=y, random_state=random_state)\n",
    "\n",
    "    # Splitting temp set into validation and test\n",
    "    test_ratio = 1 - train_ratio - val_ratio\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=random_state)\n",
    "\n",
    "    # Saving datasets\n",
    "    pd.concat([y_train, X_train], axis=1).to_csv(os.path.join(output_dir, 'train.csv'), index=False)\n",
    "    pd.concat([y_val, X_val], axis=1).to_csv(os.path.join(output_dir, 'validation.csv'), index=False)\n",
    "    pd.concat([y_test, X_test], axis=1).to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n",
    "\n",
    "    print(f\"Data split and saved in '{output_dir}':\")\n",
    "    print(f\"  - Train: {len(y_train)} samples\")\n",
    "    print(f\"  - Validation: {len(y_val)} samples\")\n",
    "    print(f\"  - Test: {len(y_test)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a7713a1-ade8-4429-b0b6-0e51b3b4b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 5572 rows and 2 columns.\n",
      "Data split and saved in '/Users/arkaroy/Downloads/sms+spam+collection/data_splits':\n",
      "  - Train: 4457 samples\n",
      "  - Validation: 557 samples\n",
      "  - Test: 558 samples\n"
     ]
    }
   ],
   "source": [
    "# Define file paths (Modify as needed)\n",
    "sms_data_file_path = \"/Users/arkaroy/Downloads/sms+spam+collection/SMSSpamCollection\"\n",
    "output_directory = \"/Users/arkaroy/Downloads/sms+spam+collection/data_splits\"\n",
    "\n",
    "# Load dataset\n",
    "sms_raw_data = load_sms_spam_dataset(sms_data_file_path)\n",
    "\n",
    "# Process dataset\n",
    "if sms_raw_data is not None:\n",
    "    sms_processed_data = preprocess_dataset(sms_raw_data)\n",
    "    split_and_save_data(sms_processed_data, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47c597-9cf2-4e1c-9fdc-5e0ccad1f1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
